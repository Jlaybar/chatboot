{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Backend ChatGPT - Jupyter Notebook\n",
    "Backend completo para conectar con la API de ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (0.104.1)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: openai in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from fastapi) (2.5.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from fastapi) (4.12.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.1 in c:\\users\\jaime.lopez\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn openai python-dotenv requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuraci√≥n cargada correctamente\n",
      "üîë API Key: ‚úÖ Configurada\n",
      "üåê Puerto: 8000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Configuraci√≥n\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or input(\"Ingresa tu API Key de OpenAI: \")\n",
    "PORT = 8000\n",
    "\n",
    "# Inicializar cliente OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n cargada correctamente\")\n",
    "print(f\"üîë API Key: {'‚úÖ Configurada' if OPENAI_API_KEY else '‚ùå No configurada'}\")\n",
    "print(f\"üåê Puerto: {PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clases y modelos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelos de datos definidos\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class QuestionRequest(BaseModel):\n",
    "    question: str\n",
    "    model: str = \"gpt-3.5-turbo\"\n",
    "    max_tokens: int = 500\n",
    "    temperature: float = 0.7\n",
    "\n",
    "class QuestionResponse(BaseModel):\n",
    "    answer: str\n",
    "    model: str\n",
    "    tokens_used: int\n",
    "\n",
    "print(\"‚úÖ Modelos de datos definidos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funci√≥n principal para ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatgpt(question: str, model: str = \"gpt-3.5-turbo\", max_tokens: int = 500, temperature: float = 0.7):\n",
    "    \"\"\"\n",
    "    Funci√≥n para hacer preguntas a la API de ChatGPT\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"ü§ñ Enviando pregunta a {model}...\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Eres un asistente √∫til y amigable.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        tokens_used = response.usage.total_tokens\n",
    "        \n",
    "        print(f\"‚úÖ Respuesta recibida ({tokens_used} tokens)\")\n",
    "        \n",
    "        return QuestionResponse(\n",
    "            answer=answer,\n",
    "            model=model,\n",
    "            tokens_used=tokens_used\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prueba directa de la funci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Enviando pregunta a gpt-3.5-turbo...\n",
      "‚úÖ Respuesta recibida (113 tokens)\n",
      "\n",
      "==================================================\n",
      "üìù PREGUNTA:\n",
      "Explique qu√© es la inteligencia artificial en 2 l√≠neas\n",
      "\n",
      "üí¨ RESPUESTA:\n",
      "La inteligencia artificial es la capacidad de las m√°quinas de realizar tareas que normalmente requieren inteligencia humana, como el aprendizaje, la percepci√≥n y la toma de decisiones, a trav√©s de algoritmos y modelos computacionales. Es un campo de la inform√°tica que busca simular la inteligencia humana en las m√°quinas.\n",
      "\n",
      "üìä METADATA:\n",
      "Modelo: gpt-3.5-turbo\n",
      "Tokens usados: 113\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Prueba directa sin servidor\n",
    "test_question = \"Explique qu√© es la inteligencia artificial en 2 l√≠neas\"\n",
    "\n",
    "result = ask_chatgpt(test_question)\n",
    "\n",
    "if result:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìù PREGUNTA:\")\n",
    "    print(test_question)\n",
    "    print(\"\\nüí¨ RESPUESTA:\")\n",
    "    print(result.answer)\n",
    "    print(\"\\nüìä METADATA:\")\n",
    "    print(f\"Modelo: {result.model}\")\n",
    "    print(f\"Tokens usados: {result.tokens_used}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backend FastAPI (ejecutable desde notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Aplicaci√≥n FastAPI creada\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import threading\n",
    "\n",
    "# Crear aplicaci√≥n FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"ChatGPT API Backend\",\n",
    "    description=\"Backend para conectar con ChatGPT\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"ChatGPT API Backend funcionando desde Jupyter\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"service\": \"chatgpt-backend\"}\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "async def ask_question(request: QuestionRequest):\n",
    "    result = ask_chatgpt(\n",
    "        question=request.question,\n",
    "        model=request.model,\n",
    "        max_tokens=request.max_tokens,\n",
    "        temperature=request.temperature\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        return result\n",
    "    else:\n",
    "        return {\"error\": \"No se pudo obtener respuesta de ChatGPT\"}\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n",
    "\n",
    "print(\"‚úÖ Aplicaci√≥n FastAPI creada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ejecutar servidor en segundo plano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Servidor iniciado en http://localhost:8000\n",
      "üìã Endpoints disponibles:\n",
      "   GET  http://localhost:8000/\n",
      "   GET  http://localhost:8000/health\n",
      "   POST http://localhost:8000/ask\n",
      "\n",
      "‚ö†Ô∏è  El servidor se detendr√° cuando cierres el notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [28216]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el servidor en un hilo separado\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(f\"üöÄ Servidor iniciado en http://localhost:{PORT}\")\n",
    "print(\"üìã Endpoints disponibles:\")\n",
    "print(f\"   GET  http://localhost:{PORT}/\")\n",
    "print(f\"   GET  http://localhost:{PORT}/health\")\n",
    "print(f\"   POST http://localhost:{PORT}/ask\")\n",
    "print(\"\\n‚ö†Ô∏è  El servidor se detendr√° cuando cierres el notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prueba del servidor desde el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Probando servidor...\n",
      "\n",
      "INFO:     127.0.0.1:54831 - \"GET /health HTTP/1.1\" 200 OK\n",
      "‚úÖ Health check: {'status': 'healthy', 'service': 'chatgpt-backend'}\n",
      "ü§ñ Enviando pregunta a gpt-3.5-turbo...\n",
      "‚úÖ Respuesta recibida (52 tokens)\n",
      "INFO:     127.0.0.1:54833 - \"POST /ask HTTP/1.1\" 200 OK\n",
      "‚úÖ Pregunta respondida correctamente\n",
      "üìù Respuesta: La capital de Espa√±a es Madrid. ¬øHay algo m√°s en lo que te pueda ayudar?\n",
      "üîß Modelo: gpt-3.5-turbo\n",
      "üî¢ Tokens: 52\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Esperar a que el servidor est√© listo\n",
    "time.sleep(2)\n",
    "\n",
    "def test_server():\n",
    "    base_url = f\"http://localhost:{PORT}\"\n",
    "    \n",
    "    print(\"üß™ Probando servidor...\\n\")\n",
    "    \n",
    "    # Probar health check\n",
    "    try:\n",
    "        health_response = requests.get(f\"{base_url}/health\")\n",
    "        print(f\"‚úÖ Health check: {health_response.json()}\")\n",
    "    except:\n",
    "        print(\"‚ùå Health check fall√≥\")\n",
    "        return\n",
    "    \n",
    "    # Probar endpoint de pregunta\n",
    "    test_data = {\n",
    "        \"question\": \"¬øCu√°l es la capital de Espa√±a?\",\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{base_url}/ask\", json=test_data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"‚úÖ Pregunta respondida correctamente\")\n",
    "            print(f\"üìù Respuesta: {result['answer']}\")\n",
    "            print(f\"üîß Modelo: {result['model']}\")\n",
    "            print(f\"üî¢ Tokens: {result['tokens_used']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error en la petici√≥n: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al conectar con el servidor: {e}\")\n",
    "\n",
    "# Ejecutar prueba\n",
    "test_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interfaz interactiva para hacer preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776c2db11fa34429af9e951617dd704b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>ü§ñ Chat con ChatGPT</h3>'), Textarea(value='', description='Pregunta:', layout=L‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def create_chat_interface():\n",
    "    \"\"\"Interfaz gr√°fica para hacer preguntas\"\"\"\n",
    "    \n",
    "    # Widgets\n",
    "    question_input = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Escribe tu pregunta aqu√≠...',\n",
    "        description='Pregunta:',\n",
    "        layout=widgets.Layout(width='80%', height='100px')\n",
    "    )\n",
    "    \n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo-preview'],\n",
    "        value='gpt-3.5-turbo',\n",
    "        description='Modelo:'\n",
    "    )\n",
    "    \n",
    "    ask_button = widgets.Button(\n",
    "        description='Preguntar a ChatGPT',\n",
    "        button_style='success',\n",
    "        tooltip='Hacer pregunta'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_ask_button_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            if not question_input.value.strip():\n",
    "                print(\"‚ùå Por favor escribe una pregunta\")\n",
    "                return\n",
    "            \n",
    "            print(\"‚è≥ Procesando pregunta...\")\n",
    "            \n",
    "            result = ask_chatgpt(\n",
    "                question=question_input.value,\n",
    "                model=model_dropdown.value\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"üí¨ RESPUESTA:\")\n",
    "                print(result.answer)\n",
    "                print(\"\\nüìä Informaci√≥n:\")\n",
    "                print(f\"   Modelo: {result.model}\")\n",
    "                print(f\"   Tokens usados: {result.tokens_used}\")\n",
    "                print(\"=\"*60)\n",
    "    \n",
    "    ask_button.on_click(on_ask_button_clicked)\n",
    "    \n",
    "    # Display interface\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>ü§ñ Chat con ChatGPT</h3>\"),\n",
    "        question_input,\n",
    "        model_dropdown,\n",
    "        ask_button,\n",
    "        output\n",
    "    ]))\n",
    "\n",
    "# Mostrar interfaz interactiva\n",
    "create_chat_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardar configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo .env creado/actualizado\n",
      "üìÅ El notebook est√° listo para usar\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"‚úÖ Archivo .env creado/actualizado\")\n",
    "print(\"üìÅ El notebook est√° listo para usar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
